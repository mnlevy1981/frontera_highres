#! /usr/bin/env python
import click

import os
import yaml

from subprocess import PIPE, Popen, check_output

from itertools import product

import time
from datetime import datetime, timezone

import xarray as xr
import numpy as np

import pop_tools


USER = os.environ['USER']

# set paths
gridfile_directory = f'/glade/work/{USER}/esmlab-regrid'
#esmlab.config.set({'regrid.gridfile-directory' : gridfile_directory})

dirwork = f'/glade/work/{USER}/cesm_inputdata/work'
os.makedirs(dirwork, exist_ok=True)   

dirout = f'/glade/work/{USER}/cesm_inputdata'
os.makedirs(dirout, exist_ok=True)   

inputdata = '/glade/p/cesmdata/cseg/inputdata'

POP_grids = ['POP_tx0.1v3', 'POP_gx3v7', 'POP_gx1v7']

nlon_pacific_xsection = {
    'POP_gx3v7': 70,
    'POP_gx1v7': 200,
    'POP_tx0.1v3': 3000,
}

nlat_acc_xsection = {
    'POP_gx3v7': 10,
    'POP_gx1v7': 45,
    'POP_tx0.1v3': 470,
}


git_repo = (check_output(['git', 'config', '--get', 'remote.origin.url'])
            .strip()
            .decode("utf-8")
            .replace('git@github.com:', 'https://github.com/')
            .replace('.git', '')            
           )

class timer(object):
    def __init__(self, name='timer'):
        self.name = name

    def __enter__(self):
        self.start = time.time()

    def __exit__(self, type, value, traceback):
        print(f'[{self.name}]: {(time.time() - self.start):0.2f} sec')


def latlon_to_scrip(nx, ny, lon0=-180., grid_imask=None, file_out=None):
    """Generate a SCRIP grid file for a regular lat x lon grid.
    
    Parameters
    ----------
    
    nx : int
       Number of points in x (longitude).
    ny : int
       Number of points in y (latitude).
    lon0 : float, optional [default=-180]
       Longitude on lefthand grid boundary.
    grid_imask : array-like, optional [default=None]       
       If the value is set to 0 for a grid point, then that point is
       considered masked out and won't be used in the weights 
       generated by the application. 
    file_out : string, optional [default=None]
       File to which to write the grid.

    Returns
    -------
    
    ds : xarray.Dataset
       The grid file dataset.       
    """
    
    # compute coordinates of regular grid
    dx = 360. / nx
    dy = 180. / ny
    lat = np.arange(-90. + dy / 2., 90., dy)
    lon = np.arange(lon0 + dx / 2., lon0 + 360., dx)

    # make 2D
    y_center = np.broadcast_to(lat[:, None], (ny, nx))
    x_center = np.broadcast_to(lon[None, :], (ny, nx))

    # compute corner points: must be counterclockwise
    y_corner = np.stack((y_center - dy / 2.,  # SW
                         y_center - dy / 2.,  # SE
                         y_center + dy / 2.,  # NE
                         y_center + dy / 2.), # NW
                        axis=2)

    x_corner = np.stack((x_center - dx / 2.,  # SW
                         x_center + dx / 2.,  # SE
                         x_center + dx / 2.,  # NE
                         x_center - dx / 2.), # NW
                        axis=2)

    # compute area
    y0 = np.sin(y_corner[:, :, 0] * np.pi / 180.) # south
    y1 = np.sin(y_corner[:, :, 3] * np.pi / 180.) # north
    x0 = x_corner[:, :, 0] * np.pi / 180.         # west
    x1 = x_corner[:, :, 1] * np.pi / 180.         # east
    grid_area = (y1 - y0) * (x1 - x0)
    
    # sum of area should be equal to area of sphere
    np.testing.assert_allclose(grid_area.sum(), 4.*np.pi)
    
    # construct mask
    if grid_imask is None:
        grid_imask = np.ones((ny, nx), dtype=np.int32)
    
    # generate output dataset
    dso = xr.Dataset()    
    dso['grid_dims'] = xr.DataArray(np.array([nx, ny], dtype=np.int32), 
                                    dims=('grid_rank',)) 
    dso.grid_dims.encoding = {'dtype': np.int32}

    dso['grid_center_lat'] = xr.DataArray(y_center.reshape((-1,)), 
                                          dims=('grid_size'),
                                          attrs={'units': 'degrees'})

    dso['grid_center_lon'] = xr.DataArray(x_center.reshape((-1,)), 
                                          dims=('grid_size'),
                                          attrs={'units': 'degrees'})
    
    dso['grid_corner_lat'] = xr.DataArray(y_corner.reshape((-1, 4)), 
                                          dims=('grid_size', 'grid_corners'), 
                                          attrs={'units': 'degrees'})
    dso['grid_corner_lon'] = xr.DataArray(x_corner.reshape((-1, 4)), 
                                      dims=('grid_size', 'grid_corners'), 
                                      attrs={'units': 'degrees'})    

    dso['grid_imask'] = xr.DataArray(grid_imask.reshape((-1,)), 
                                     dims=('grid_size'),
                                     attrs={'units': 'unitless'})
    dso.grid_imask.encoding = {'dtype': np.int32}
    
    dso['grid_area'] = xr.DataArray(grid_area.reshape((-1,)), 
                                     dims=('grid_size'),
                                     attrs={'units': 'radians^2',
                                            'long_name': 'area weights'})
    
    # force no '_FillValue' if not specified
    for v in dso.variables:
        if '_FillValue' not in dso[v].encoding:
            dso[v].encoding['_FillValue'] = None

    dso.attrs = {'title': f'{dy} x {dx} (lat x lon) grid',
                 'created_by': 'latlon_to_scrip',
                 'date_created': f'{datetime.now()}',
                 'conventions': 'SCRIP',
                }
            
    # write output file
    if file_out is not None:
        print(f'writing {file_out}')
        dso.to_netcdf(file_out)
        
    return dso


def file_name_topo(product):
    if product == 'etopo1':
        return '/glade/work/mclong/etopo1/ETOPO1_Ice_c_gmt4.nc'
    else:
        raise ValueError(f'unknown topography dataset: {product}')

        
def file_name_weight(src, dst, method):
    """get the name of a weight file for source and destination grids"""
    return f'{gridfile_directory}/weights/{src}_to_{dst}_{method}.nc'


def file_name_grid(grid_name):
    return f'{gridfile_directory}/{grid_name}.nc'


def sedfrac_file(grid):
    return f'{dirwork}/sedfrac.{grid}.nc'


def file_name_pop_topography(grid_name):
    grid_attrs = pop_tools.grid.grid_defs[grid_name]
    return pop_tools.grid.INPUTDATA.fetch(
        grid_attrs['topography_fname'], 
        downloader=pop_tools.grid.downloader
    )


def compute_topo_adjacent_points(dst_grid):
    """
    Find points adjacent to topography and return 3D DataArray 
    (nz x nlat x nlon) that is set to "1" at points adjacent to topography.
    
    "Topography adjacent" is defined here as points where the ocean model's bottom
    of a neighbor cell is higher in the water column.       
    """
    
    # grid properties
    ds = pop_tools.get_grid(dst_grid)
    nk = len(ds.z_t)
    nj, ni = ds.KMT.shape
    ltripole = ds.attrs['type'] == 'tripole'
    
    # mask out cells where k is below KMT
    K = K_DataArray(nk, nj, ni)
    MASK = K.where(K < ds.KMT)
    MASK = xr.where(MASK.notnull(), True, False)
    
    # Construct an array of `KMT` at all eight adjoining grid cells. 
    # `xarray.DataArray.roll` shifts the data periodically. For the `lon` 
    # direction, the domain is periodic, so this is appropriate. Our 
    # Greenland-pole grids have land at both northern and southern parts of 
    # the logical domain, so no special treament is required. The tri-pole 
    # grid, however, is periodic: the left half of the top row maps to the 
    # right half. If `ltripole == True`, we replace the bottom row of the 
    # `KMT` array with the top row flipped left-to-right. 
    kmt_neighbors = xr.DataArray(np.empty((nj, ni, 8)), dims=('nlat', 'nlon', 'neighbor'))
    neighbor = []
    kmt_rollable = ds.KMT.copy()
    if ltripole:
        kmt_rollable[0, :] = kmt_rollable[-1, ::-1]

    n = 0
    for iroll, jroll in product([-1, 0, 1], [-1, 0, 1]):
        # rolling?
        if iroll == 0 and jroll == 0: continue

        # directional coordinate label
        i = '' if iroll == 0 else 'W' if iroll < 0 else 'E'
        j = '' if jroll == 0 else 'N' if jroll < 0 else 'S'
        neighbor.append(j+i)

        # record kmt
        kmt_neighbors[:, :, n] = kmt_rollable.roll(nlat=jroll, nlon=iroll, roll_coords=False)
        n += 1

    side_wall_present = (kmt_neighbors <= ds.KMT).any(dim='neighbor')
    side_wall_kmt_min = (kmt_neighbors).min(dim='neighbor').astype(np.int)

    # compute topo adjacent points
    # points where a neighbors KMT is less than mine, set topo_adjacent = 1 
    # for k range between my KMT and the neighbor's KMT that is highest in the water column
    # (lowest value of KMT)
    topo_adjacent = xr.zeros_like(MASK)
    topo_adjacent = topo_adjacent.where(
        (side_wall_present & (K < ds.KMT) & (K >= side_wall_kmt_min))
    )
    topo_adjacent = xr.where(topo_adjacent.notnull(), 1, 0)
    topo_adjacent.name = 'topo_adjacent_points'
    
    return topo_adjacent.where(MASK).fillna(0)
    

def apply_topo_adj_sedfrac_min(
    sedfrac, land_adjacent, land_adj_sedfrac_min
):
    """apply the `land_adj_sedfrac_min` to a sedfrac field"""
    return xr.where(
        (land_adjacent == 1) & (sedfrac < land_adj_sedfrac_min), 
        land_adj_sedfrac_min, 
        sedfrac
    )


def K_DataArray(nk, nj, ni):
    zero_to_km_m1 = xr.DataArray(np.arange(0, nk), dims=('z_t'))
    ONES_3d = xr.DataArray(np.ones((nk, nj, ni)), dims=('z_t', 'nlat', 'nlon'))
    return (zero_to_km_m1 * ONES_3d)


def get_3d_ocean_mask(dst_grid):
    """return a 3D ocean mask, a DataArray set to "True" for valid
       ocean points.
    """
    ds = pop_tools.get_grid(dst_grid)        
    nk = len(ds.z_t)
    nj, ni = ds.KMT.shape

    K = K_DataArray(nk, nj, ni)
    MASK = K.where(K < ds.KMT)

    return xr.where(MASK.notnull(), True, False)


def remap_z_t(da, src_grid, dst_grid):
    """Handle vertical "interpolation", exploiting the fact 
       that the POP vertical coordinates are identical for 
       all grids, except that the hi-res model has 2 extra 
       levels at the bottom. 
    """
    
    grids_60lev = ['POP_gx1v6', 'POP_gx1v7', 'POP_gx3v7']
    grids_62lev = ['POP_tx0.1v3']
    
    new_z_t = pop_tools.get_grid(dst_grid).z_t

    # lo-res ---> lo-res: just return
    if src_grid in grids_60lev and dst_grid in grids_60lev:
        return da.assign_coords({'z_t': new_z_t})
                   
    # hi-res ---> lo-res: return all but the lowest two levels
    elif src_grid in grids_62lev and dst_grid in grids_60lev:
        return da.isel(z_t=slice(0, -2)).assign_coords({'z_t': new_z_t})

    # lo-res ---> hi-res: copy the lowest level down
    elif src_grid in grids_60lev and dst_grid in grids_62lev:
        dao_slab_m2 = da.isel(z_t=-1).assign_coords({'z_t': new_z_t[-2]})
        dao_slab_m1 = da.isel(z_t=-1).assign_coords({'z_t': new_z_t[-1]})
        return xr.concat((da, dao_slab_m2, dao_slab_m1), dim=new_z_t)
    else:
        raise ValueError(f'unknown grid combination: {src_grid} --> {dst_grid}')

        
def ncks_fl_fmt64bit(file):
    """
    Converts file to netCDF-3 64bit by calling:
      ncks --fl_fmt=64bit
    
    Parameter
    ---------
    file : str
      The file to convert.
    """

    ncks_cmd = ' '.join(['ncks', '-O', '--fl_fmt=64bit', file, file])
    cmd = ' && '.join(['module load nco', ncks_cmd])
    
    p = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)
    
    stdout, stderr = p.communicate()
    if p.returncode != 0:
        print(stdout.decode('UTF-8'))
        print(stderr.decode('UTF-8'))
        raise   

    
def to_netcdf_clean(dset, path, format='NETCDF3_64BIT', **kwargs):
    """wrap to_netcdf method to circumvent some xarray shortcomings"""
    
    dset = dset.copy()
    
    # ensure _FillValues are not added where they don't exist
    for v in dset.variables:
        if '_FillValue' not in dset[v].encoding:
            dset[v].encoding['_FillValue'] = None


    git_sha = check_output(['git', 'describe', '--always']).strip().decode("utf-8")
    datestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    provenance_str = f'created by {git_repo}/tree/{git_sha} on {datestamp}'

    if 'history' in dset.attrs:
        dset.attrs['history'] += '; ' + provenance_str
    else:
        dset.attrs['history'] = provenance_str

    print('-'*30)
    print(f'Writing {path}')
    dset.info()
    print()
    dset.to_netcdf(path, format=format, **kwargs)    